# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19bRig9kK0ptacKG5yKtaBHGGgD0W8FS2
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import os
import time
from tqdm import tqdm
from copy import deepcopy

# Define paths for Google Colab without Drive mount
MODEL_PATH = "/content/model.pth"  # Assuming model is uploaded directly to Colab session
SAVE_DIR = "/content/pruning_results"  # Results will be saved to Colab session
os.makedirs(SAVE_DIR, exist_ok=True)

# Pruning percentages to test
PRUNING_PERCENTAGES = [0, 25, 50, 75, 90]

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

class SinusoidalPositionEmbeddings(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim

    def forward(self, time):
        device = time.device
        half_dim = self.dim // 2
        embeddings = torch.log(torch.tensor(10000, device=device)) / (half_dim - 1)
        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)
        embeddings = time[:, None] * embeddings[None, :]
        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)
        return embeddings

class Block(nn.Module):
    def __init__(self, in_channels, out_channels, time_emb_dim, up=False):
        super().__init__()
        self.time_mlp =  nn.Linear(time_emb_dim, out_channels)
        if up:
            self.conv1 = nn.Conv2d(2 * in_channels, out_channels, 3, padding=1)
            self.transform = nn.ConvTranspose2d(out_channels, out_channels, 4, 2, 1)
        else:
            self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)
            self.transform = nn.Conv2d(out_channels, out_channels, 4, 2, 1)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
        self.bnorm1 = nn.BatchNorm2d(out_channels)
        self.bnorm2 = nn.BatchNorm2d(out_channels)
        self.relu  = nn.ReLU()

    def forward(self, x, t):
        # First Conv
        h = self.bnorm1(self.relu(self.conv1(x)))
        # Time embedding
        time_emb = self.relu(self.time_mlp(t))
        # Extend last 2 dimensions
        time_emb = time_emb[(..., ) + (None, ) * 2]
        # Add time channel
        h = h + time_emb
        # Second Conv
        h = self.bnorm2(self.relu(self.conv2(h)))
        # Down or Upsample
        return self.transform(h)

class SimpleUnet(nn.Module):
    """
    A simplified variant of the Unet architecture.
    """
    def __init__(self):
        super().__init__()
        image_channels = 3
        down_channels = (64, 128, 256, 512, 1024)
        up_channels = (1024, 512, 256, 128, 64)
        out_dim = 3
        time_emb_dim = 32

        # Time embedding
        self.time_mlp = nn.Sequential(
                SinusoidalPositionEmbeddings(time_emb_dim),
                nn.Linear(time_emb_dim, time_emb_dim),
                nn.ReLU()
            )

        # Initial projection
        self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1)

        # Downsample
        self.downs = nn.ModuleList([Block(down_channels[i], down_channels[i+1], \
                                    time_emb_dim) \
                    for i in range(len(down_channels)-1)])
        # Upsample
        self.ups = nn.ModuleList([Block(up_channels[i], up_channels[i+1], \
                                        time_emb_dim, up=True) \
                    for i in range(len(up_channels)-1)])

        self.output = nn.Conv2d(up_channels[-1], out_dim, 1)

    def forward(self, x, timestep):
        # Embedd time
        t = self.time_mlp(timestep)
        # Initial conv
        x = self.conv0(x)
        # Unet
        residual_inputs = []
        for down in self.downs:
            x = down(x, t)
            residual_inputs.append(x)
        for up in self.ups:
            residual_x = residual_inputs.pop()
            # Add residual x as additional channels
            x = torch.cat((x, residual_x), dim=1)
            x = up(x, t)
        return self.output(x)

# Load the pretrained model
def load_model(model_path):
    print(f"Loading model from {model_path}")

    # Load the state dictionary
    state_dict = torch.load(model_path, map_location=device)

    # Assuming you know your model architecture, create an instance of it
    # Replace "YourModelClass" with the actual class of your model
    model = SimpleUnet()

    # Load the state dictionary into the model
    model.load_state_dict(state_dict)

    model.to(device)
    model.eval()
    return model

def get_flowers102_dataset():
    transform = transforms.Compose([
        transforms.Resize((64, 64)),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

    test_dataset = datasets.Flowers102(
        root='/content/data',  # Colab temporary storage
        split='test',
        transform=transform,
        download=True
    )

    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)
    return test_loader

def generate_images(model, starting_image=None, num_images=5, img_size=64, channels=3, timesteps=1000):
        model.eval()
        with torch.no_grad():
            if starting_image is None:
                # Start with random noise if no starting image is provided
                x = torch.randn(num_images, channels, img_size, img_size).to(device)
            else:
                # Use the starting image
                x = starting_image.unsqueeze(0).to(device)  # Add batch dimension

            # Denoise gradually
            for t in tqdm(range(timesteps-1, -1, -1), desc="Generating images"):
                t_batch = torch.tensor([t] * num_images).to(device)
                x = model(x, t_batch)

            # Normalize to [0, 1] for visualization
            x = (x + 1) / 2
            x = torch.clamp(x, 0, 1)

        return x

# Calculate FID score (simplified version)
def calculate_fid(real_images, generated_images):
    # This is a simplified FID calculation - in practice, you'd use a proper FID implementation
    # using the Inception network features

    # Convert to numpy arrays
    real = real_images.reshape(real_images.shape[0], -1).cpu().numpy()
    gen = generated_images.reshape(generated_images.shape[0], -1).cpu().numpy()

    # Calculate mean and covariance
    mu_real = np.mean(real, axis=0)
    sigma_real = np.cov(real, rowvar=False)

    mu_gen = np.mean(gen, axis=0)
    sigma_gen = np.cov(gen, rowvar=False)

    # Calculate FID
    mu_diff = mu_real - mu_gen

    # Calculate trace term
    trace_term = np.trace(sigma_real + sigma_gen - 2*np.sqrt(sigma_real @ sigma_gen))

    # Add small epsilon to avoid numerical issues
    epsilon = 1e-6
    fid = mu_diff.dot(mu_diff) + trace_term + epsilon

    return fid

# Evaluate the model
def evaluate_model(model, test_loader, starting_image):
    # Generate images using the starting image
    generated_images = generate_images(model, starting_image=starting_image)

    # Get some real images from the test set
    real_images = next(iter(test_loader))[0][:5].to(device)

    # Calculate FID score
    fid_score = calculate_fid(real_images, generated_images)

    # Calculate SSIM (structural similarity index)
    # This is a simplified placeholder - in practice, use a proper SSIM implementation
    ssim_score = 0.0

    return {
        "fid": fid_score,
        "ssim": ssim_score,
        "generated_images": generated_images
    }

# Magnitude-based pruning
def prune_model_magnitude(model, prune_percentage):
    pruned_model = deepcopy(model)

    # Get all parameters
    all_weights = []
    for name, param in pruned_model.named_parameters():
        if 'weight' in name:
            all_weights.append((name, param))

    # Calculate threshold in chunks to reduce memory usage
    thresholds = []
    for _, param in all_weights:
        thresholds.append(torch.quantile(param.view(-1).abs(), prune_percentage / 100))
    threshold = torch.mean(torch.tensor(thresholds, device=param.device)) # Average thresholds

    # Apply pruning
    for name, param in all_weights:
        mask = param.abs() > threshold
        param.data = param.data * mask

    print(f"Applied {prune_percentage}% magnitude pruning")
    return pruned_model

# Hilbert-Schmidt pruning
def prune_model_hilbert_schmidt(model, prune_percentage):
    pruned_model = deepcopy(model)

    # Get all parameters
    all_weights = []
    for name, param in pruned_model.named_parameters():
        if 'weight' in name and len(param.shape) > 1:  # Only consider matrices
            all_weights.append((name, param))

    # Calculate Hilbert-Schmidt norm for each weight matrix
    hs_norms = []
    for name, param in all_weights:
        # Hilbert-Schmidt norm is the Frobenius norm
        hs_norm = torch.norm(param, p='fro')
        hs_norms.append((name, param, hs_norm))

    # Sort by importance (lower norm = less important)
    hs_norms.sort(key=lambda x: x[2])

    # Calculate how many parameters to prune
    total_params = sum(p.numel() for _, p, _ in hs_norms)
    params_to_prune = int(total_params * prune_percentage / 100)

    # Prune the least important parameters
    params_pruned = 0
    for name, param, _ in hs_norms:
        if params_pruned >= params_to_prune:
            break

        mask = torch.zeros_like(param, dtype=torch.bool)
        param.data = param.data * mask
        params_pruned += param.numel()

    print(f"Applied {prune_percentage}% Hilbert-Schmidt pruning")
    return pruned_model

# Save generated images
def save_images(images, filename):
    plt.figure(figsize=(15, 5))
    for i in range(min(5, len(images))):
        plt.subplot(1, 5, i+1)
        plt.imshow(images[i].cpu().permute(1, 2, 0))
        plt.axis('off')
    plt.tight_layout()
    plt.savefig(filename)
    plt.close()

# Main function to run all experiments
def run_pruning_experiments():
    # Load the model
    model = load_model(MODEL_PATH)

    # Get the test dataset
    test_loader = get_flowers102_dataset()
    real_images, _ = next(iter(test_loader))
    starting_image = real_images[0]
    # Results dictionary
    results = {
        "imp": {},
        "hilbert_schmidt": {}
    }

    # Run experiments for each pruning method and percentage
    for method in ["imp", "hilbert_schmidt"]:
        print(f"\nRunning {method.upper()} pruning experiments:")

        for percentage in PRUNING_PERCENTAGES:
            print(f"\nPruning percentage: {percentage}%")

            # Apply pruning
            if percentage == 0:
                pruned_model = deepcopy(model)
            elif method == "imp":
                pruned_model = prune_model_magnitude(model, percentage)
            else:
                pruned_model = prune_model_hilbert_schmidt(model, percentage)

            # Evaluate the pruned model
            eval_results = evaluate_model(pruned_model, test_loader, starting_image)

            # Save results
            results[method][percentage] = {
                "fid": eval_results["fid"],
                "ssim": eval_results["ssim"]
            }

            # Save generated images
            save_images(
                eval_results["generated_images"],
                f"{SAVE_DIR}/{method}_{percentage}_percent.png"
            )

            # Save model
            torch.save(pruned_model, f"{SAVE_DIR}/{method}_{percentage}_percent.pth")

    # Plot and save results
    plot_results(results)

    return results

# Plot and save results
def plot_results(results):
    # Plot FID scores
    plt.figure(figsize=(10, 6))
    for method in results:
        percentages = sorted(results[method].keys())
        fid_scores = [results[method][p]["fid"] for p in percentages]
        plt.plot(percentages, fid_scores, marker='o', label=f"{method.upper()}")

    plt.xlabel("Pruning Percentage")
    plt.ylabel("FID Score (lower is better)")
    plt.title("FID Score vs Pruning Percentage")
    plt.legend()
    plt.grid(True)
    plt.savefig(f"{SAVE_DIR}/fid_scores.png")
    plt.close()

    # Print table of results
    print("\nResults Summary:")
    print("-" * 60)
    print(f"{'Method':<20} {'Pruning %':<10} {'FID':<10} {'SSIM':<10}")
    print("-" * 60)

    for method in results:
        for percentage in sorted(results[method].keys()):
            fid = results[method][percentage]["fid"]
            ssim = results[method][percentage]["ssim"]
            print(f"{method.upper():<20} {percentage:<10} {fid:<10.4f} {ssim:<10.4f}")

# Add a function to allow downloading results
def download_results():
    from google.colab import files
    import zipfile
    import os

    # Create a zip file of all results
    zip_path = '/content/pruning_results.zip'
    with zipfile.ZipFile(zip_path, 'w') as zipf:
        for root, dirs, files in os.walk(SAVE_DIR):
            for file in files:
                zipf.write(os.path.join(root, file),
                           os.path.relpath(os.path.join(root, file),
                                           os.path.join(SAVE_DIR, '..')))

    # Download the zip file
    files.download(zip_path)
    print(f"Downloaded results as {zip_path}")

w# Run the experiments
if __name__ == "__main__":
    start_time = time.time()
    results = run_pruning_experiments()
    end_time = time.time()

    print(f"\nTotal execution time: {end_time - start_time:.2f} seconds")
    print(f"Results saved in {SAVE_DIR}")

    # Uncomment the line below to download results after running
    download_results()